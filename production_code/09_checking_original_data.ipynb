{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking data\n",
    "\n",
    "Along the way of pivotting and cleaning, we should make sure our data hasnt been 'destroyed'\n",
    "\n",
    "this file is used to go through all stages of the data and pivot it the same way to make sure, at every stage, is the data in the same shape or have we lost a lot of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = '../'\n",
    "sys.path.insert(1, '../production_code/')\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  imports original acident and node data\n",
    "accidents = pd.read_csv(ROOT_DIR + ACCIDENT_DATA_GENERAL_DIR)\n",
    "node = pd.read_csv(ROOT_DIR + ACCIDENT_DATA_NODE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting base accidents by date and day of week\n",
    "accidents.loc[:,'ACCIDENTDATE'] = pd.to_datetime(accidents.loc[:,'ACCIDENTDATE'], format=\"%d/%m/%Y\")\n",
    "accidents.loc[:,'DAY_OF_WEEK'] = accidents.loc[:,'ACCIDENTDATE'].dt.day_of_week"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing all the data stages\n",
    "\n",
    "by grouping and pivotting the data, any stage can be broken down into \n",
    "- the total number of calls made each day, averaged over all days in a certain date range. \n",
    "- then groupped by region.\n",
    "\n",
    "so the values you see are the average total number of daily callouts for each region\n",
    "\n",
    "This way we can make sure the average number of emergency calls made in each region per day, stays constant accross all dates, as it should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting what date range we are looking at\n",
    "date_min = pd.to_datetime(TRAIN_SPLIT_MIN_DATE) #TRAIN_SPLIT_MIN_DATE\n",
    "date_max = pd.to_datetime(TEST_TRAIN_SPLIT_DATE) #TEST_SPLIT_MAX_DATE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Original\n",
    "\n",
    "comparing with the very original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small transformation to police value\n",
    "accidents.loc[:,'POLICE_ATTEND'] = accidents['POLICE_ATTEND'].apply(lambda x: 2 - x)\n",
    "\n",
    "# calculates and prints data\n",
    "original_pivot = accidents\\\n",
    "     .merge(\n",
    "         # first merges with location data \n",
    "          node[['ACCIDENT_NO','REGION_NAME']]\\\n",
    "               .drop_duplicates(), \n",
    "          how = 'inner'\n",
    "     #and drops duplicates\n",
    "          )\\\n",
    "     .drop_duplicates(subset=['ACCIDENT_NO'])\\\n",
    "     .query('ACCIDENTDATE >= @date_min & ACCIDENTDATE < @date_max')\\\n",
    "     .pivot_table(    # sums the total number of police attended for each day, splitting by region\n",
    "         index = ['ACCIDENTDATE','REGION_NAME'],\n",
    "         values=['POLICE_ATTEND'],\n",
    "         aggfunc='sum',\n",
    "         # averages police attened over all days\n",
    "         )\\\n",
    "     .reset_index()\\\n",
    "     .groupby(['REGION_NAME'])['POLICE_ATTEND']\\\n",
    "     .mean()\\\n",
    "     .sort_index()\n",
    "\n",
    "# displays data\n",
    "original_pivot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original with basic filtering\n",
    "\n",
    "same original data but with some of the basic filtering lines added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents = accidents.dropna(subset=['ACCIDENTDATE'])\n",
    "accidents = accidents.dropna(subset=['ACCIDENTTIME'])\n",
    "# accidents = accidents.query(\"LIGHT_CONDITION != 9\")\n",
    "accidents = accidents.query(\"POLICE_ATTEND != 9\")   # removing when unsure if police attened or not\n",
    "accidents = accidents[pd.to_datetime(accidents['ACCIDENTDATE']) > pd.to_datetime(EARLIEST_DATE)].reset_index(drop = True)\n",
    "\n",
    "# calculates and prints data\n",
    "# first merges with location data and drops duplicates\n",
    "accidents\\\n",
    "    .merge(\n",
    "         # first merges with location data \n",
    "        node[['ACCIDENT_NO','REGION_NAME']]\\\n",
    "            .drop_duplicates(), \n",
    "        how = 'inner'\n",
    "     #and drops duplicates\n",
    "        )\\\n",
    "    .drop_duplicates(subset=['ACCIDENT_NO'])\\\n",
    "    .query('ACCIDENTDATE >= @date_min & ACCIDENTDATE < @date_max')\\\n",
    "    .pivot_table(    # sums the total number of police attended for each day, splitting by region\n",
    "         index = ['ACCIDENTDATE','REGION_NAME'],\n",
    "         values=['POLICE_ATTEND'],\n",
    "         aggfunc='sum'\n",
    "         # averages police attened over all days for each region\n",
    "         )\\\n",
    "    .reset_index()\\\n",
    "    .groupby(['REGION_NAME'])['POLICE_ATTEND']\\\n",
    "    .mean()\\\n",
    "    .sort_index() \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roughly cleaned pre merge data\n",
    "\n",
    "this is data still from the initial transofmring, before it was merged with other rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the data and applies some transformations\n",
    "first_clean = pd.read_csv(ROOT_DIR + ROUGHLY_CLEANED_PRE_MERGE_DATA_DIR)\n",
    "first_clean.loc[:,'POLICE_ATTEND'] = first_clean['POLICE_ATTEND'].apply(lambda x: 2 - x)\n",
    "first_clean.loc[:,'date'] = pd.to_datetime(first_clean.loc[:,'date']).dt.date\n",
    "\n",
    "\n",
    "# calculates and prints data\n",
    "# first merges with location data and drops duplicates\n",
    "first_clean\\\n",
    "    .merge(node[['ACCIDENT_NO','REGION_NAME']]\\\n",
    "        .drop_duplicates(),  how = 'inner')\\\n",
    "    .query('date >= @date_min & date < @date_max')\\\n",
    "    .pivot_table(    # sums the total number of police attended for each day, splitting by region\n",
    "         index = ['date','REGION_NAME'],\n",
    "         values=['POLICE_ATTEND'],\n",
    "         aggfunc='sum'\n",
    "         # averages police attened over all days for each region\n",
    "         )\\\n",
    "    .reset_index()\\\n",
    "    .groupby(['REGION_NAME'])['POLICE_ATTEND']\\\n",
    "    .mean()\\\n",
    "    .sort_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing to initial clean\n",
    "\n",
    "This data is after the full initial clean, still have similar numbers as the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads file and adds some data changes\n",
    "first_clean = pd.read_csv(ROOT_DIR + ROUGHLY_CLEANED_DATA_DIR)\n",
    "first_clean.loc[:,'date'] = pd.to_datetime(first_clean.loc[:,'date']).dt.date\n",
    "\n",
    "# calculates and prints data\n",
    "# first crops the data\n",
    "first_clean\\\n",
    "    .query('date >= @date_min & date < @date_max')\\\n",
    "    .pivot_table(    # sums the total number of police attended for each day, splitting by region\n",
    "         index = ['date','region'],\n",
    "         values=['police_needed'],\n",
    "         aggfunc='sum'\n",
    "         # averages police attened over all days for each region\n",
    "         )\\\n",
    "    .reset_index()\\\n",
    "    .groupby(['region'])['police_needed']\\\n",
    "    .mean()\\\n",
    "    .sort_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing to further clean\n",
    "\n",
    "fully cleaned data after all the visulizations and second cleaning stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads file and adds some data changes\n",
    "second_clean = pd.read_csv(ROOT_DIR + FULLY_CLEANED_DATA_DIR)\n",
    "second_clean.loc[:,'date'] = pd.to_datetime(first_clean.loc[:,'date']).dt.date\n",
    "\n",
    "# calculates and prints data\n",
    "# first crops the data\n",
    "second_clean\\\n",
    "    .query('date >= @date_min & date < @date_max')\\\n",
    "    .pivot_table(    # sums the total number of police attended for each day, splitting by region\n",
    "         index = ['date','region'],\n",
    "         values=['police_needed'],\n",
    "         aggfunc='sum'\n",
    "         # averages police attened over all days for each region\n",
    "         )\\\n",
    "    .reset_index()\\\n",
    "    .groupby(['region'])['police_needed']\\\n",
    "    .mean()\\\n",
    "    .sort_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing to pre-piviotted\n",
    "\n",
    "this is the data extracted just before completing the final pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads file and adds some data changes\n",
    "pre_pivot = pd.read_csv(ROOT_DIR + PREPIVOT_TRAIN_TEST_DATA_DIR)\n",
    "pre_pivot.loc[:,'date'] = pd.to_datetime(pre_pivot.loc[:,'date']).dt.date\n",
    "\n",
    "# calculates and prints data\n",
    "# first crops the data\n",
    "pre_pivot\\\n",
    "    .query('date >= @date_min & date < @date_max')\\\n",
    "    .pivot_table(    # sums the total number of police attended for each day, splitting by region\n",
    "         index = ['date','region'],\n",
    "         values=['police_needed'],\n",
    "         aggfunc='sum'\n",
    "         # averages police attened over all days for each region\n",
    "         )\\\n",
    "    .reset_index()\\\n",
    "    .groupby(['region'])['police_needed']\\\n",
    "    .mean()\\\n",
    "    .sort_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre pivotted with scaled police\n",
    "\n",
    "This data is the same as above, but looking at the scaled_police term instead of the police_needed term to make sure the scaling was correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculates and prints data\n",
    "# first crops the data\n",
    "pre_pivot\\\n",
    "    .query('date >= @date_min & date < @date_max')\\\n",
    "    .pivot_table(    # sums the total number of police attended for each day, splitting by region\n",
    "         index = ['date','region'],\n",
    "         values=['scaled_police'],\n",
    "         aggfunc='sum'\n",
    "         # averages police attened over all days for each region\n",
    "         )\\\n",
    "    .reset_index()\\\n",
    "    .groupby(['region'])['scaled_police']\\\n",
    "    .mean()\\\n",
    "    .sort_index()/2  # devide by two due to scaling police count to count for twice as many instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing to final data\n",
    "\n",
    "final pivotted data, very close to the original even with the massive change in format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing either training or testing data\n",
    "# file = REGION_TESTING_DATA_DIR\n",
    "file = REGION_TRAINING_DATA_DIR\n",
    "data = pd.read_csv(ROOT_DIR + file)\n",
    "\n",
    "# pivots the data\n",
    "final_pivot = data\\\n",
    "    .pivot_table(      # averaging rainging or not\n",
    "        index = ['Region','Part of Day','Day of the Week'],\n",
    "        values=['Police'],\n",
    "        aggfunc='mean'\n",
    "    )\\\n",
    "    .reset_index()\\\n",
    "    .pivot_table(      # adding together parts of day\n",
    "        index = ['Region','Day of the Week'],\n",
    "        values=['Police'],\n",
    "        aggfunc='sum'\n",
    "    )\\\n",
    "    .reset_index()\\\n",
    "    .groupby(['Region'])['Police']\\\n",
    "    .mean()\\\n",
    "    .sort_index()\n",
    "\n",
    "# prints the data\n",
    "final_pivot "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing with original\n",
    "\n",
    "This shows the percentage change between the final data and inital data\n",
    "everything being 100% would be ideal\n",
    "most are around that, overall, clearly there is a bit of lost data but nothing major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the percentage difference\n",
    "100 * final_pivot / original_pivot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
