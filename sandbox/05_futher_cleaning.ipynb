{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import sys\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "ROOT_DIR = '../'\n",
    "sys.path.insert(1, '../production_code/')\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ROOT_DIR + FULLY_CLEANED_DATA_DIR)\n",
    "\n",
    "# fixing date type\n",
    "df.loc[:,'date'] = pd.to_datetime(df.loc[:,'date'])\n",
    "\n",
    "# adding pure date stamp\n",
    "df.loc[:,'date_stamp'] =  pd.to_datetime(df.loc[:,'date'].dt.date)\n",
    "\n",
    "day_dict = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\", 4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"}\n",
    "hour_bin_dict = {0: 'Night', 1: 'Morning', 2:'Afternoon',3:'Evening'}\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Issues\n",
    "\n",
    "### initial pivot\n",
    "\n",
    "after completing an intial pivot of the data, there are two main issues\n",
    "\n",
    "1. there is no larger date grouping, so if we have 6 years of data, the calculated needed will be double that of 3 years\n",
    "2. Rainy days dont apear as often as clear days, so currently, looks like rainy days are less dangerous\n",
    "3. some less populated regions have very little need\n",
    "4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing how it pivots\n",
    "(df.pivot_table(\n",
    "    index = ['day','hour_bin','region','sky'],\n",
    "    values = ['police_needed', 'ambulance_needed'],\n",
    "    aggfunc = 'sum'\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Issue 1. scaling by time span\n",
    "\n",
    "The largest time scale is the 7 days of a week. Therefore if we had 1 year of data, there would be 52 potential weeks that would be combined into the one table. and for 3 years, 156\n",
    "\n",
    "Therefore, we simply need to divide everything by the number of weeks in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.to_datetime(df['date']).max())\n",
    "print(pd.to_datetime(df['date']).min())\n",
    "\n",
    "# small function to find num of weeks\n",
    "def calc_num_weeks_between(min, max):\n",
    "    return math.ceil(\n",
    "        (max - min).days / 7\n",
    "    )\n",
    "    \n",
    "num_of_weeks = calc_num_weeks_between(df['date'].min(), df['date'].max())\n",
    "\n",
    "# then final result must be divided by this\n",
    "num_of_weeks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 2: need to scale weather conditions\n",
    "\n",
    "If we assume that the dataset is vast enough, for each date, there would be at least one crash, with the weather reported. Therefore, by looking at the number of times it is either clear or not, for each hourly bin (removing duplicates from the smallest time group to ignore multiple crashes). We can get a rough distribution of the percentage of time it is clear, grouped by hourly bin for some extra granuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating count of times its not clear per hourly bin\n",
    "time_dist_rain = df[df['sky'] == 'Not clear']\\\n",
    "        .drop_duplicates(   # removing duplicates in smallest time group\n",
    "            subset = ['date_stamp']\n",
    "            )\\\n",
    "        .groupby(    # grouping by hourly_bin\n",
    "            ['hour_bin']\n",
    "            )['ACCIDENT_NO']\\\n",
    "        .count() \n",
    "\n",
    "\n",
    "# repeats the\n",
    "#  same for clear weather\n",
    "time_dist_clear = df[df['sky'] == 'Clear'].drop_duplicates(subset = ['date_stamp','hour_bin']).groupby(['hour_bin'])['ACCIDENT_NO'].count()\n",
    "\n",
    "# calculating percentage clear\n",
    "clear_weights = time_dist_clear / (time_dist_rain + time_dist_clear)\n",
    "clear_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to use these weights to scale\n",
    "def scale_need(row, clear_weights):\n",
    "    \n",
    "    # if its clear\n",
    "    if row['sky'] == 'Clear':\n",
    "\n",
    "        # scales based on clear weights\n",
    "        return row[0] * (1 - clear_weights[row['hour_bin']])\n",
    "    # else its raining\n",
    "    else:\n",
    "\n",
    "        # scales based on not clear weights\n",
    "        return row[0] * clear_weights[row['hour_bin']]\n",
    "\n",
    "# uses above function to scale need\n",
    "df.loc[:,'scaled_police'] = df.loc[:,['police_needed','sky','day','hour_bin']].apply(scale_need, axis = 1, clear_weights = clear_weights)\n",
    "df.loc[:,'scaled_ambulance'] = df.loc[:,['ambulance_needed','sky','day','hour_bin']].apply(scale_need, axis = 1, clear_weights = clear_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 3: low counts in regional areas\n",
    "\n",
    "No scaling makes sense here, the counts are low because the number of crashes are low.\n",
    "\n",
    "# final pivotted data\n",
    "\n",
    "looking at the data now, it looks really good, the number needed for non clear days is slightly high than clear days, more ambulances are needed in metro areas. And no time is left without an ambulance. time to generate a training and tesing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing how it pivots\n",
    "final = (df.pivot_table(\n",
    "    index = ['day','hour_bin','region','sky'],\n",
    "    values = ['scaled_police', 'scaled_ambulance'],\n",
    "    aggfunc = 'sum'\n",
    ") * 100 / num_of_weeks)\n",
    "\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_final_df(df_final, query = 'sky == \"Clear\"'):\n",
    "    df_temp = df_final.reset_index().query(query).pivot_table(\n",
    "        index = 'day',\n",
    "        columns = 'region',\n",
    "        values = 'scaled_ambulance',\n",
    "        aggfunc = 'mean',\n",
    "    ).T\n",
    "\n",
    "\n",
    "    # create a heatmap\n",
    "    fig = px.imshow(\n",
    "        df_temp, \n",
    "        x=list(day_dict.values()), \n",
    "        y=list(df_temp.index), \n",
    "        labels={\n",
    "                \"x\": \"Day of the Week\",\n",
    "                \"y\": \"region\",\n",
    "                \"color\": \"% chance of needing an ambulace\"\n",
    "            }, \n",
    "        title=\"Average Chance of an Ambulance, sky is clear\"\n",
    "        )\n",
    "\n",
    "    # add annotations\n",
    "    fig.update_traces(hoverinfo='text', text=df_temp.values, texttemplate='%{text:.2f}')\n",
    "    # show the figure\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "fig = display_final_df(final)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_date = pd.to_datetime(TRAIN_SPLIT_MIN_DATE)\n",
    "split_date = pd.to_datetime(TEST_TRAIN_SPLIT_DATE)\n",
    "max_date = pd.to_datetime(TEST_SPLIT_MAX_DATE)\n",
    "\n",
    "\n",
    "# testing how it pivots\n",
    "df_train = df\\\n",
    "    .query('date >= @min_date & date < @split_date')\\\n",
    "    .pivot_table(\n",
    "        index = ['day','hour_bin','region','sky'],\n",
    "        values = ['scaled_police', 'scaled_ambulance'],\n",
    "        aggfunc = 'sum'\n",
    "    ) / calc_num_weeks_between(min_date, split_date)\n",
    "\n",
    "\n",
    "\n",
    "# testing how it pivots\n",
    "df_test = df\\\n",
    "    .query('date >= @split_date & date < @max_date')\\\n",
    "    .pivot_table(\n",
    "        index = ['day','hour_bin','region','sky'],\n",
    "        values = ['scaled_police', 'scaled_ambulance'],\n",
    "        aggfunc = 'sum'\n",
    "    ) / calc_num_weeks_between(split_date, max_date)\n",
    "\n",
    "\n",
    "fig = display_final_df(df_train)\n",
    "fig = display_final_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing how it pivots\n",
    "df_train = df\\\n",
    "    .query('date >= @min_date & date < @split_date')\\\n",
    "    .pivot_table(\n",
    "        index = ['day','hour_bin','lga','sky'],\n",
    "        values = ['scaled_police', 'scaled_ambulance'],\n",
    "        aggfunc = 'sum'\n",
    "    ) / calc_num_weeks_between(min_date, split_date)\n",
    "\n",
    "\n",
    "\n",
    "# testing how it pivots\n",
    "df_test = df\\\n",
    "    .query('date >= @split_date & date < @max_date')\\\n",
    "    .pivot_table(\n",
    "        index = ['day','hour_bin','lga','sky'],\n",
    "        values = ['scaled_police', 'scaled_ambulance'],\n",
    "        aggfunc = 'sum'\n",
    "    ) / calc_num_weeks_between(split_date, max_date)\n",
    "\n",
    "\n",
    "# saving data\n",
    "\n",
    "df_train.to_csv(ROOT_DIR + TRAINING_DATA_DIR)\n",
    "df_test.to_csv(ROOT_DIR + TESTING_DATA_DIR)\n",
    "\n",
    "df_train.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
