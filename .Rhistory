#from solutions
par(mfrow=c(2,2))
plot(predict(model3), residuals(model3))
halfnorm(residuals(model3), ylab="residuals")
halfnorm(rstudent(model3), ylab="jackknife resid")
halfnorm(cooks.distance(model3), ylab="cooks dist")
model4 = glm(
ca ~ offset(log(cells)) + doserate * doseamt,
family = poisson(link = "log"),
data = dicentric[c(-27,-19),]
)
#comparing
confint(model3)
confint(model4)
summary(model3)
summary(model4)
#these points were causing this artifact
#interaction term is no longer significant
#remove 19, interaction term sig changes, remove 27 same thing
par(mfrow=c(2,2))
plot(predict(model4), residuals(model4))
halfnorm(residuals(model4), ylab="residuals")
halfnorm(rstudent(model4), ylab="jackknife resid")
halfnorm(cooks.distance(model4), ylab="cooks dist")
#over dispersion because of the phi value
sum(residuals(model3, type="pearson")^2)/23
#got 12, should be 1 for this model. therefore over dispersion. should use a quasi model not normal model
#ploting expected values of ca vs real value of ca (expectation of a poisson is labda)
par(mfrow=c(1,1))
dicentric[19,]
dicentric[,1]
plot(exp(predict(model3)),dicentric[,2])
m = lm(exp(predict(model3)) ~ 0 + dicentric[,2])
abline(m)
summary(m)
x = seq(0,10,0.1)
plot(x,x)
par(mfrow=c(2,2))
plot(predict(model4), residuals(model4))
halfnorm(residuals(model4), ylab="residuals")
halfnorm(rstudent(model4), ylab="jackknife resid")
halfnorm(cooks.distance(model4), ylab="cooks dist")
#from solutions
par(mfrow=c(2,2))
plot(predict(model3), residuals(model3))
halfnorm(residuals(model3), ylab="residuals")
halfnorm(rstudent(model3), ylab="jackknife resid")
halfnorm(cooks.distance(model3), ylab="cooks dist")
plot(model2)
par(mfrow=c(2,2))
plot(predict(model3), residuals(model3))
halfnorm(residuals(model3), ylab="residuals")
halfnorm(rstudent(model3), ylab="jackknife resid")
halfnorm(cooks.distance(model3), ylab="cooks dist")
summary(model3)
summary(model4)
sum(residuals(model3, type="pearson")^2)/23
dicentric
sum(residuals(model3, type="pearson")^2)/23
sum(residuals(model4, type="pearson")^2)/23
#poisson
poism = step(glm(
numDefects + 1 ~ (prebake + flux + cooling + temp)^2,
family = poisson(link = "log"),
data = wavesolder[-27,]
), trace = 0)
#summary(poism)
summary(poism)$dispersion
(phi <- sum(residuals(poism, type="pearson")^2) / poism$df.residual)
rmal
#quassi poisson
poisfull = glm(
numDefects + 1~ (prebake + flux + cooling + temp)^2,
family = quasipoisson(link = "log"),
data = wavesolder[-27,]
)
summary(poisfull)
#quassi poisson
poisfull = glm(
numDefects + 1~ (prebake + flux + cooling + temp)^2,
family = quasipoisson(link = "log"),
data = wavesolder[-27,]
)
summary(poisfull)
famlily?
?family
link-glm
?link-glm
?family
betareg
library(survival)
data(infert)
?infert
str(infert)
model1 <- glm(
cbind( case, 1 - case) ~ . -pooled.stratum -stratum,
family = binomial,
data = infert)
summary(model1)
#testing sig levle of education
#remove education
modeln <- glm(
cbind( case, 1 - case) ~ . -pooled.stratum -stratum -education,
family = binomial,
data = infert)
#modeln <- glm(
#  cbind( case, 1 - case) ~ 1,
#  family = binomial,
#  data = infert)
#compare deviance and df
dev <- modeln$deviance - model1$deviance
n <- modeln$df.residual - model1$df.residual
#chisq test
pchisq(dev, n, lower.tail = FALSE)
anova(model1, modeln)
#questions 3
data("discoveries")
?discoveries
start = 1860
finish = 1959
#transforming the data
dis <- discoveries[1:(finish - start + 1)]
yer <- seq(start, finish)
data <- matrix(c(yer,dis), nrow = (finish - start + 1), ncol = 2)
#with just year
model1 <- glm(
dis ~ yer,
family = poisson(link = "log")
)
summary(model1)
#modle with year and year^2
model2 <- glm(
dis ~ yer + I(yer^2),
family = poisson(link = "log"),
trace = 0
)
summary(model2)
#CODE FROM SOLUTIONS TO GRAPH
x <- disc.df$year
plot(x, disc.df$disc)
beta1 <- model1$coefficients
lines(x, exp(beta1[1] + beta1[2]*x), col="blue", lty=2)
beta2 <- model2$coefficients
lines(x, exp(beta2[1] + beta2[2]*x + beta2[3]*x^2), col="red")
#END OF GRAPHING CODE
#null model
modeln <- glm(
dis ~ 1,
family = poisson(link = "log"),
trace = 0
)
#compare deviance and df
anova(modeln, model1, model2)
#chisq tests
#year to year^2
(dev <- model1$deviance - model2$deviance )
n <- model1$df.residual - model2$df.residual
pchisq(dev, n, lower.tail = FALSE)
#year to null
(dev <- modeln$deviance - model1$deviance)
n <- modeln$df.residual - model1$df.residual
pchisq(dev, n, lower.tail = FALSE)
#year^2 to null
(dev <- modeln$deviance - model2$deviance)
n <- modeln$df.residual - model2$df.residual
pchisq(dev, n, lower.tail = FALSE)
#compare deviance and df
anova(modeln, model1, model2)
#compare deviance and df
anova(modeln, model1, model2, test='F')
#compare deviance and df
anova(modeln, model1, model2, test='t')
#compare deviance and df
anova(modeln, model1, model2, test='Chisq')
anova(model1, test="Chisq")
library(faraway)
data(cornnit)
?cornnit
cornnit
x = log(cornnit$nitrogen + 1)
y = (cornnit$yield)
plot(x, y)
y
summary(glmfit)
# 1a
#check week 4 lab doc for def of pearson res
pearson.res = residuals(glmfit, type="pearson")
#calcing dispersion parameter:
(disp = sum(pearson.res^2)/glmfit$df.residual)
anova(glmfit)
anova(glmfit,test='F')
anova(glmfit,test='Chisq')
y = log(cornnit$yield+5)
x = log(cornnit$nitrogen + 5)
y = cornnit$yield
plot(x,y)
plot(exp(x)-5,y)
?family
plot(x,y)
wavesolder
wavesolder["temp"]
log(wavesolder["temp"])
log(wavesolder["temp"] + 1)
apply(wavesolder["temp"], function(x) log(x)
)
apply(wavesolder["temp"], log(x))
sapply(wavesolder["temp"], log(x))
sapply(wavesolder["temp"], log
)
sapply(wavesolder["temp"], 1, log(x))
sapply(wavesolder["temp"], 1, log())
sapply(wavesolder["temp"], 1, log
)
sapply(wavesolder["temp"], 1, log)
lapply(wavesolder["temp"], log)
lapply(wavesolder["temp"], log(x))
lapply(as.list(wavesolder["temp"]), log)
lapply((wavesolder["temp"]), log)
lapply((wavesolder["temp"]))
log(2)
lapply((wavesolder["temp"]), log(x))
lapply((wavesolder["temp"]), max
)
lapply((wavesolder["temp"]), max)
wavesolder["temp"]
log(wavesolder["temp"])
wavesolder["temp"].log
apply(log,wavesolder["temp"])
library(faraway)
data(dicentric)
with(dicentric, interaction.plot(doseamt, doserate, ca/cells))
?dicentric
dicentric
# can model counts or rates
model = glm(
ca ~ offset(log(cells)) + doserate + doseamt,
family = poisson(link = "log"),
data = dicentric
)
par(mfrow=c(2,2))
plot(model)
dicentric
dicentric[14,]
model2 = glm(
ca ~ offset(log(cells)) + doserate + doseamt,
family = poisson(link = "log"),
data = dicentric[c(-19, -9, -27),]
)
par(mfrow=c(2,2))
plot(model2)
model2
summary(model2)
model2 = glm(
ca ~ offset(log(cells)) + doseamt,
family = poisson(link = "log"),
data = dicentric[c(-19, -9, -27),]
)
model3 = glm(
ca ~ offset(log(cells)) + doserate + doseamt,
family = poisson(link = "log"),
data = dicentric[c(-19, -9, -27),]
)
anova(model2, model3)
anova(model2, model3, test="F")
anova(model2, model3, test="chisq")
anova(model2, model3, test="Chisq")
?predict
# setting up working spaces
library(faraway)
setwd("D:/projects/Crash-Stats-Vic-data")
#backwards selection formula
back_selection <- function(base_formula, family, data, test = "F", sig_level = 0.05){
to_remove = c()
# repeats until code breaking
repeat{
# if there are variables to remove
if(length(to_remove)){
# remakes the formula
formula = paste0(
base_formula ,
" -", paste(to_remove, collapse = " -"))
}else{
# otherwise keeps the formula
formula = base_formula
}
# creates the model
model = glm(
as.formula(formula),
family = family,
data = data
)
# uses the drop1 function for finding what should be removed
drop_scores = drop1(model, test=test)
# if test is and f test
if(test == "F"){
# finds what had the least sig score
highest_pr = max(drop_scores$`Pr(>F)`[-1])
# finds the name of the features
removing = rownames(drop_scores)[which.max(drop_scores$`Pr(>F)`)]
# adds to the remove list
to_remove = c(to_remove, removing)
# repeats the same if a chi test is needed
}else if(test == "Chi"){
highest_pr = max(drop_scores$`Pr(>Chi)`[-1])
removing = rownames(drop_scores)[which.max(drop_scores$`Pr(>Chi)`)]
to_remove = c(to_remove, removing)
# removed t test code
}#else if(test == "t"){
#  highest_pr = max(drop_scores$`Pr(>|t|)`[-1])
#  removing = rownames(drop_scores)[which.max(drop_scores$`Pr(>Chi)`)]
#  to_remove = c(to_remove, removing)
#}
# if the least sig value is greater than the break point
if(highest_pr < sig_level){
# prints and brekas
print(cat('breaking, highest pr is: ', highest_pr))
break
}
# otherwise prints whats being removed and continues
print(cat('removing: ',removing, highest_pr))
}
# returns the final model once done
return(model)
}
# setting up working spaces
library(faraway)
setwd("D:/projects/Crash-Stats-Vic-data")
#backwards selection formula
back_selection <- function(base_formula, family, data, test = "F", sig_level = 0.05){
to_remove = c()
# repeats until code breaking
repeat{
# if there are variables to remove
if(length(to_remove)){
# remakes the formula
formula = paste0(
base_formula ,
" -", paste(to_remove, collapse = " -"))
}else{
# otherwise keeps the formula
formula = base_formula
}
# creates the model
model = glm(
as.formula(formula),
family = family,
data = data
)
# uses the drop1 function for finding what should be removed
drop_scores = drop1(model, test=test)
# if test is and f test
if(test == "F"){
# finds what had the least sig score
highest_pr = max(drop_scores$`Pr(>F)`[-1])
# finds the name of the features
removing = rownames(drop_scores)[which.max(drop_scores$`Pr(>F)`)]
# adds to the remove list
to_remove = c(to_remove, removing)
# repeats the same if a chi test is needed
}else if(test == "Chi"){
highest_pr = max(drop_scores$`Pr(>Chi)`[-1])
removing = rownames(drop_scores)[which.max(drop_scores$`Pr(>Chi)`)]
to_remove = c(to_remove, removing)
# removed t test code
}#else if(test == "t"){
#  highest_pr = max(drop_scores$`Pr(>|t|)`[-1])
#  removing = rownames(drop_scores)[which.max(drop_scores$`Pr(>Chi)`)]
#  to_remove = c(to_remove, removing)
#}
# if the least sig value is greater than the break point
if(highest_pr < sig_level){
# prints and brekas
print(cat('breaking, highest pr is: ', highest_pr))
break
}
# otherwise prints whats being removed and continues
print(cat('removing: ',removing, highest_pr))
}
# returns the final model once done
return(model)
}
# importing data
data_train <- read.csv(file ="data/clean/train_region.csv", header=TRUE)
data_test <- read.csv(file ="data/clean/test_region.csv", header=TRUE)
data_train
# outlier list
outliers = c(276,278)
# small value for greater than 0 cases
epsilon = 0.0000001
# factoring categorical data
data_train$Part.of.Day <- factor(data_train$Part.of.Day )
data_train$Day.of.the.Week <- factor(data_train$Day.of.the.Week)
data_train$Region <- factor(data_train$Region)
data_train$Sky <- factor(data_train$Sky)
#inital plot
gammam = glm(
Police + epsilon ~ (Region + Part.of.Day + Sky + Day.of.the.Week) ^ 2,
family = Gamma(link = "inverse"),
data = data_train
)
# diagnostic plots
par(mfrow=c(2,2))
plot(gammam)
# more diagnostic plots
par(mfrow=c(2,2))
halfnorm(residuals(gammam), ylab="residuals", main = 'Residuals half-norm')
halfnorm(rstudent(gammam), ylab="jackknife residuals", main = 'Jackknife residuals')
# diagnostic plots
par(mfrow=c(2,2))
plot(gammam)
# diagnostic plots
par(mfrow=c(2,2))
plot(gammam)
# more diagnostic plots
par(mfrow=c(2,2))
halfnorm(residuals(gammam), ylab="residuals", main = 'Residuals half-norm')
halfnorm(rstudent(gammam), ylab="jackknife residuals", main = 'Jackknife residuals')
plot(gammam, which = 1)
halfnorm(cooks.distance(gammam), ylab="cooks dist", col = c('orange3') , main = 'Cooks distance')
# summary
summary(gammam)
# using backwards selection to remove insig features
gammam = back_selection(
"Police + epsilon ~ Region + (Day.of.the.Week + Part.of.Day + Sky)^2",
family = Gamma(link = "log"),
data = data_train[-outliers,],
sig_level = 0.05,
test = "Chi"
)
# printing final formula and summary after back selections
gammam$formula
summary(gammam)
# final models
# police model
gammam_police = glm(
"Police + epsilon ~ Region + (Day.of.the.Week + Part.of.Day + Sky)^2",
family = Gamma(link = "log"),
data = data_train[-outliers,]
)
# ambulance model
gammam_ambulance = glm(
"Ambulance + epsilon ~ Region + (Day.of.the.Week + Part.of.Day + Sky)^2",
family = Gamma(link = "log"),
data = data_train[-outliers,]
)
# pritning summary again
summary(gammam_police)
#comparing the dispersion, doesnt matter for gamma but still
summary(gammam)$dispersion
(phi <- sum(residuals(gammam, type="pearson")^2) / gammam$df.residual)
# diagnostic plots again
par(mfrow=c(2,2))
halfnorm(residuals(gammam_police), ylab="residuals", main = 'Residuals half-norm')
halfnorm(rstudent(gammam_police), ylab="jackknife residuals", , main = 'Jackknife residuals')
plot(gammam_police, which = 1)
halfnorm(cooks.distance(gammam_police), ylab="cooks dist", col = c('orange3') , main = 'Cooks distance')
# predicting test values
# predict police incidents for data_test
police_predicted = predict(gammam_police, newdata = data_test, type = "response") - epsilon
data_test$police_predicted = police_predicted
# subtracting epsilone from y values
ppy = (gammam_police$fitted.values) - epsilon
ppx = (gammam_police$y) - epsilon
# plotting police data
par(mfrow=c(1,2))
# plotting police training data first
plot(
ppy ~ ppx,
col = 'blue',
xlab = "Training True Police",
ylab = "Predicted")
abline (a = 0, b = 1)     # Target line
sqrt(mean((ppy - ppx)^2))   # RMSE
# plotting test data second
plot(police_predicted ~ Police,
data_test,
col = 'darkblue',
xlab = "Testing True Police",
ylab = "Predicted"
)
abline (a = 0, b = 1)    # Target line
sqrt(mean((police_predicted - data_test$Police)^2))    # RMSE
# predicting ambulance data
abulance_predicted = predict(gammam_ambulance, newdata = data_test, type = "response") - epsilon
data_test$abulance_predicted = abulance_predicted
# subtracting epsilone from y values
pay = (gammam_ambulance$fitted.values - epsilon)
pax = (gammam_ambulance$y - epsilon)
# plotting ambulance data
par(mfrow=c(1,2))
plot(
pay ~ pax,
col = 'red',
xlab = "Training True Ambulance",
ylab = "Predicted")
abline (a = 0, b = 1)     # Target line
sqrt(mean((pay - pax)^2))    # RMSE
# plotting test data second
plot(abulance_predicted ~ Ambulance,
data_test,
col = 'darkred',
xlab = "Testing True Ambulance",
ylab = "Predicted")
abline (a = 0, b = 1)      # Target line
sqrt(mean((abulance_predicted - data_test$Ambulance)^2))    # RMSE
# viewing top 5, ordering by true value, then ordering by training values
head(data_test[order(data_test$Police, decreasing = TRUE),], n = 5)
head(data_test[order(data_test$police_predicted, decreasing = TRUE),], n = 5)
summary(gammam)
# viewing top 5, ordering by true value, then ordering by training values
head(data_test[order(data_test$Police, decreasing = TRUE),], n = 5)
head(data_test[order(data_test$police_predicted, decreasing = TRUE),], n = 5)
